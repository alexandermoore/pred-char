{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense, LSTM, Embedding\n",
    "from keras.models import Model\n",
    "from keras.backend import clear_session\n",
    "from keras.optimizers import Adam\n",
    "from functions import *\n",
    "import string\n",
    "import random\n",
    "VALID_CHARS = set([x for x in (string.ascii_lowercase + \"0123456789\" + '.!?,-\" ')])\n",
    "REPLACE_CHARS = {\n",
    "    \";\": '.',\n",
    "    '\\n': ' '\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncate_text_len = 1000000\n",
    "norm_text = normalize_text(load_text(\"obama.txt\"), VALID_CHARS, REPLACE_CHARS)[:truncate_text_len]\n",
    "split_idx = int(len(norm_text) * 0.80)\n",
    "norm_text_train = norm_text[:split_idx]\n",
    "norm_text_val = norm_text[split_idx:]\n",
    "vocab, inv_vocab = build_vocab_map(VALID_CHARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 100, 128)          5504      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100, 256)          394240    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 256)          0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100, 44)           11308     \n",
      "=================================================================\n",
      "Total params: 411,052\n",
      "Trainable params: 411,052\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "clear_session()\n",
    "sample_len = 100\n",
    "batch_size = 64\n",
    "hidden_dim = 256\n",
    "embedding_dim = 128\n",
    "num_lstms = 1\n",
    "max_vocab_index = len(vocab)\n",
    "optimizer = Adam(lr=1e-3)\n",
    "model = build_lstm_model(sample_len, num_lstms, hidden_dim, embedding_dim, vocab)\n",
    "print(max_vocab_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800000 200000 44\n",
      "(' have an empathy deficit when were still sending our children down corridors of shame, schools in th', 'have an empathy deficit when were still sending our children down corridors of shame, schools in the')\n",
      "('those of us who manage the publics dollars will be held to account -- to spend wisely, reform bad ha', 'hose of us who manage the publics dollars will be held to account -- to spend wisely, reform bad hab')\n",
      "('or every american family that is paying the price at the pump -- we must end this dependence on fore', 'r every american family that is paying the price at the pump -- we must end this dependence on forei')\n",
      "(' and reduce premiums for medicare beneficiaries by roughly 43 billion over the next 10 years. and im', 'and reduce premiums for medicare beneficiaries by roughly 43 billion over the next 10 years. and im ')\n",
      "('ree suspected terrorists. let me repeat that three convictions in over seven years. instead of bring', 'ee suspected terrorists. let me repeat that three convictions in over seven years. instead of bringi')\n",
      "('l qaeda attack, it is just as likely, if not more, that it will be here in europe in a european city', ' qaeda attack, it is just as likely, if not more, that it will be here in europe in a european city.')\n",
      "('nt to move in the same direction towards a better future for our children and our grandchildren. and', 't to move in the same direction towards a better future for our children and our grandchildren. and ')\n",
      "(' will likely be made up of 35-50,000 u.s. troops. through this period of transition, we will carry o', 'will likely be made up of 35-50,000 u.s. troops. through this period of transition, we will carry ou')\n",
      "('ing intrinsically wrong in us taking better care of ourselves. but what accounts for the bulk of our', 'ng intrinsically wrong in us taking better care of ourselves. but what accounts for the bulk of our ')\n",
      "('he way for europes renaissance and enlightenment. it was innovation in muslim communities -- it was ', 'e way for europes renaissance and enlightenment. it was innovation in muslim communities -- it was i')\n"
     ]
    }
   ],
   "source": [
    "print(len(norm_text_train), len(norm_text_val), len(vocab))\n",
    "testgen=sample_text(norm_text_train, sample_len, vocab)\n",
    "for _ in range(10):\n",
    "    print(next(testgen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "12500/12500 [==============================] - 1181s 95ms/step - loss: 1.3117 - acc: 0.6024 - val_loss: 1.2638 - val_acc: 0.6162\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f84d4de6470>"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.fit_generator(generator=sample_text(norm_text, 40),\n",
    "#                    validation_data=None,\n",
    "#                    use_multiprocessing=True,\n",
    "#                    workers=6)\n",
    "\n",
    "model.fit_generator(generator=sample_batch(norm_text_train, sample_len, batch_size, vocab), \n",
    "                    steps_per_epoch=int(len(norm_text_train)/batch_size),\n",
    "                    validation_data=sample_batch(norm_text_val, sample_len, batch_size, vocab, shuffle=False),\n",
    "                    validation_steps=int(len(norm_text_val)/batch_size),\n",
    "                    epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'americans and the recovery plan that will be the democratic contrality that we should be a new tradition of a conflict of the world with the moment when we are not a served that it was the future of the co'"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = normalize_text(\"ameri\", VALID_CHARS, REPLACE_CHARS)\n",
    "generate_text(model, txt, vocab, inv_vocab, sample_len, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 5, 42)\n",
      "(3, 5, 42)\n",
      "(3, 5, 42)\n",
      "(3, 5, 42)\n",
      "(3, 5, 42)\n",
      "(3, 5, 42)\n",
      "(3, 5, 42)\n",
      "(3, 5, 42)\n",
      "(3, 5, 42)\n",
      "(3, 5, 42)\n"
     ]
    }
   ],
   "source": [
    "text = \"abcdefgh\"\n",
    "samp = \"xyz\"\n",
    "generator1 = sample_text(text, 5)\n",
    "generator2 = sample_batch(text, 5, 3, vocab)\n",
    "for i in range(10):\n",
    "    samp = next(generator2)\n",
    "    print(samp[1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('s and they are many.', ' and they are many. ')"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator1 = sample_text(norm_text, 20)\n",
    "next(generator1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
